
# ğŸ¤ Social Sidekick

**Social Sidekick** is a real-time voice-to-AI assistant that listens to your speech, transcribes it using Whisper, and responds with helpful, witty responses generated by the LLaMA 3 model running locally with [Ollama](https://ollama.com). Itâ€™s ideal for social practice, conversational exploration, or just having a clever sidekick by your side.

---

## âœ¨ Features

- ğŸ—£ï¸ Record your voice using a microphone
- ğŸ§ Press `Enter` to start and stop listening
- ğŸ“ Transcribe speech to text using [Faster Whisper](https://github.com/SYSTRAN/faster-whisper)
- ğŸ¤– Generate smart, funny, or friendly responses using `llama3` via Ollama
- ğŸªµ Real-time console logging to guide your interactions

---

## ğŸ–¥ï¸ Requirements

- Windows, macOS, or Linux
- Python 3.8 or later
- A working microphone
- [Ollama](https://ollama.com/download) installed and running
- Basic terminal usage

---

## ğŸ›  Installation Instructions

### 1. Clone the Project

```bash
git clone https://github.com/yourusername/social-sidekick.git
cd social-sidekick
```

### 2. Set Up Python Virtual Environment (Recommended)

```bash
python -m venv venv
```

- **Windows:**

  ```bash
  venv\Scripts\activate
  ```

- **macOS/Linux:**

  ```bash
  source venv/bin/activate
  ```

### 3. Install Required Python Packages

```bash
pip install sounddevice scipy numpy keyboard faster-whisper ollama
```

> If you run into permission errors, try `pip install --upgrade pip` first.

### 4. Install and Run Ollama

- Download Ollama from [ollama.com/download](https://ollama.com/download)
- Pull the LLaMA 3 model:

```bash
ollama pull llama3
```

- Start the model server:

```bash
ollama run llama3
```

Keep this terminal open in the background while running the Python script.

---

## ğŸš€ How to Use

### Run the Application

```bash
python SocialSidekick.py
```

### Interaction Flow

1. You'll see:

    ```
    ğŸ” Loading Whisper model...
    ğŸ™ï¸ Press [Enter] to start listening.
    ```

2. Press **Enter** â†’ Speak â†’ Press **Enter again** to stop.

3. The app will:

    - Transcribe what you said
    - Send it to LLaMA 3 for a response
    - Show the result like this:

    ```
    ğŸ‘‚ Heard: What do I say when someone invites me to a group dinner?
    ğŸ’¬ Sidekick: Just say "Thanks for the invite! I'd love to join. What's the plan?"
    ```

4. It waits for you to press **Enter** again to repeat the cycle.

---

## ğŸ“· Example Output

```
ğŸ” Loading Whisper model...
ğŸ™ï¸ Press [Enter] to start listening.
ğŸ§ Listening... Press [Enter] again to stop.
ğŸ›‘ Stopped listening.

ğŸ‘‚ Heard: I'm not sure how to say hi to people I don't know.
ğŸ¤– Generating reply...

ğŸ’¬ Sidekick: Start with a warm â€œHey there! Iâ€™m [Your Name], nice to meet you!â€ Itâ€™s classic, friendly, and always works.
```

---

## ğŸ§© Customization

You can adjust the following:

- **Whisper model type** (`"base"`, `"small"`, `"medium"`, `"large"`)
  ```python
  whisper = WhisperModel("base", compute_type="int8", device="cpu")
  ```

- **Sidekick personality**
  Edit the `"system"` message in `chat_with_llama()`:
  ```python
  "You're a helpful and witty social sidekick. Help the user fit in..."
  ```

---

## ğŸ›  Troubleshooting

### â— `ModuleNotFoundError: No module named 'ollama'`

Install it:

```bash
pip install ollama
```

### â— `OSError: No Default Input Device Available`

Make sure:
- Your microphone is connected and not being used by other apps.
- Youâ€™re not running in a headless environment (like SSH without audio).

### â— `Ollama model not found`

Pull the model manually:

```bash
ollama pull llama3
```

---

## ğŸ§‘â€ğŸ’» Author

Made with ğŸ’¬ by [Your Name] â€” inspired by the challenge of being social in a noisy world.

---

## ğŸ“œ License

This project is licensed under the MIT License. Use freely, share widely.

---

## ğŸ™‹ FAQ

**Q:** Can I use a different model like Mistral or Gemma with Ollama?  
**A:** Yes! Just change the `model='llama3'` line to another supported model name in `chat_with_llama()`.

**Q:** Does this run offline?  
**A:** Yes! Both Whisper (with CPU inference) and Ollama run locally. No cloud API keys needed.
