
# 🎤 Social Sidekick

**Social Sidekick** is a real-time voice-to-AI assistant that listens to your speech, transcribes it using Whisper, and responds with helpful, witty responses generated by the LLaMA 3 model running locally with [Ollama](https://ollama.com). It’s ideal for social practice, conversational exploration, or just having a clever sidekick by your side.

---

## ✨ Features

- 🗣️ Record your voice using a microphone
- 🎧 Press `Enter` to start and stop listening
- 📝 Transcribe speech to text using [Faster Whisper](https://github.com/SYSTRAN/faster-whisper)
- 🤖 Generate smart, funny, or friendly responses using `llama3` via Ollama
- 🪵 Real-time console logging to guide your interactions

---

## 🖥️ Requirements

- Windows, macOS, or Linux
- Python 3.8 or later
- A working microphone
- [Ollama](https://ollama.com/download) installed and running
- Basic terminal usage

---

## 🛠 Installation Instructions

### 1. Clone the Project

```bash
git clone https://github.com/yourusername/social-sidekick.git
cd social-sidekick
```

### 2. Set Up Python Virtual Environment (Recommended)

```bash
python -m venv venv
```

- **Windows:**

  ```bash
  venv\Scripts\activate
  ```

- **macOS/Linux:**

  ```bash
  source venv/bin/activate
  ```

### 3. Install Required Python Packages

```bash
pip install sounddevice scipy numpy keyboard faster-whisper ollama
```

> If you run into permission errors, try `pip install --upgrade pip` first.

### 4. Install and Run Ollama

- Download Ollama from [ollama.com/download](https://ollama.com/download)
- Pull the LLaMA 3 model:

```bash
ollama pull llama3
```

- Start the model server:

```bash
ollama run llama3
```

Keep this terminal open in the background while running the Python script.

---

## 🚀 How to Use

### Run the Application

```bash
python SocialSidekick.py
```

### Interaction Flow

1. You'll see:

    ```
    🔁 Loading Whisper model...
    🎙️ Press [Enter] to start listening.
    ```

2. Press **Enter** → Speak → Press **Enter again** to stop.

3. The app will:

    - Transcribe what you said
    - Send it to LLaMA 3 for a response
    - Show the result like this:

    ```
    👂 Heard: What do I say when someone invites me to a group dinner?
    💬 Sidekick: Just say "Thanks for the invite! I'd love to join. What's the plan?"
    ```

4. It waits for you to press **Enter** again to repeat the cycle.

---

## 📷 Example Output

```
🔁 Loading Whisper model...
🎙️ Press [Enter] to start listening.
🎧 Listening... Press [Enter] again to stop.
🛑 Stopped listening.

👂 Heard: I'm not sure how to say hi to people I don't know.
🤖 Generating reply...

💬 Sidekick: Start with a warm “Hey there! I’m [Your Name], nice to meet you!” It’s classic, friendly, and always works.
```

---

## 🧩 Customization

You can adjust the following:

- **Whisper model type** (`"base"`, `"small"`, `"medium"`, `"large"`)
  ```python
  whisper = WhisperModel("base", compute_type="int8", device="cpu")
  ```

- **Sidekick personality**
  Edit the `"system"` message in `chat_with_llama()`:
  ```python
  "You're a helpful and witty social sidekick. Help the user fit in..."
  ```

---

## 🛠 Troubleshooting

### ❗ `ModuleNotFoundError: No module named 'ollama'`

Install it:

```bash
pip install ollama
```

### ❗ `OSError: No Default Input Device Available`

Make sure:
- Your microphone is connected and not being used by other apps.
- You’re not running in a headless environment (like SSH without audio).

### ❗ `Ollama model not found`

Pull the model manually:

```bash
ollama pull llama3
```

---

## 🧑‍💻 Author

Made with 💬 by [Your Name] — inspired by the challenge of being social in a noisy world.

---

## 📜 License

This project is licensed under the MIT License. Use freely, share widely.

---

## 🙋 FAQ

**Q:** Can I use a different model like Mistral or Gemma with Ollama?  
**A:** Yes! Just change the `model='llama3'` line to another supported model name in `chat_with_llama()`.

**Q:** Does this run offline?  
**A:** Yes! Both Whisper (with CPU inference) and Ollama run locally. No cloud API keys needed.
